# AWS S3 Labs: Complete Guide from Basic to Advanced
## With Practical Examples, Testing & Full Cleanup Procedures

---

## Overview

This guide contains 16 progressive labs covering AWS S3 from foundational to enterprise-level scenarios. Each lab includes objectives, step-by-step instructions, practical file operations, and complete cleanup procedures.

**Topics Covered:**
- S3 Bucket Creation & Management
- Object Upload/Download/Delete
- Access Control & Permissions
- Versioning & Lifecycle Policies
- Encryption & Security
- Static Website Hosting
- CloudFront Integration
- S3 Event Notifications
- Replication & Backup
- Monitoring & Optimization
- Cost Management

---

## SECTION 1: S3 FUNDAMENTALS (Labs 1-4)

### Lab 1: Creating Your First S3 Bucket

**Objective:** Create an S3 bucket and understand bucket naming conventions.

**Why This Matters:**
- S3 = Simple Storage Service (object storage)
- Buckets = Containers for objects
- Bucket names must be globally unique
- Foundation for all S3 operations

### Understanding S3

```
S3 Structure:

AWS Account
â””â”€ S3 Service
   â”œâ”€ Bucket 1 (my-photos)
   â”‚  â”œâ”€ photo1.jpg (Object)
   â”‚  â”œâ”€ photo2.jpg (Object)
   â”‚  â””â”€ vacation/
   â”‚     â””â”€ beach.jpg (Object with prefix)
   â”‚
   â”œâ”€ Bucket 2 (my-backups)
   â”‚  â”œâ”€ database.sql
   â”‚  â””â”€ documents.zip
   â”‚
   â””â”€ Bucket 3 (my-logs)
      â””â”€ app-logs-2024.txt

Each bucket = Unique global namespace
Each object = File with metadata
```

### Step-By-Step Instructions

**Step 1: Navigate to S3 Console**
- Open AWS Console
- Search for "S3"
- Click "S3" service
- Click "Create Bucket"

**Step 2: Configure Bucket**
- Bucket name: `my-first-s3-bucket-[RANDOM]`
  - Must be globally unique
  - Lowercase letters, numbers, hyphens only
  - 3-63 characters long
  - Cannot start/end with hyphen
  - Example: `my-first-s3-bucket-20240101`

- Region: us-east-1 (closest to you)
- Click "Create Bucket"

**Step 3: Understand Bucket Properties**

After creation, view:
- Bucket name (globally unique)
- Region (where data is stored)
- Creation date
- Access (currently private)
- Versioning (disabled by default)
- Default encryption (not enabled)

**Step 4: Enable Useful Features**

Click on bucket â†’ Properties:
- Edit "Block Public Access":
  - Keep all blocks ON for now (secure by default)
- Edit "Versioning":
  - Enable versioning (ability to recover old versions)
- Edit "Default encryption":
  - Enable AES-256 encryption (FREE)

**Step 5: Explore Bucket Structure**

Current view shows:
- Objects tab (empty now)
- Properties tab
- Permissions tab
- Lifecycle tab
- etc.

### Bucket Naming Rules

```
VALID Names:
âœ… my-bucket
âœ… my-bucket-2024
âœ… 123bucket
âœ… my-very-long-bucket-name-with-dashes

INVALID Names:
âŒ My-Bucket (uppercase)
âŒ my_bucket (underscores not allowed)
âŒ my bucket (spaces not allowed)
âŒ mybucket- (ends with hyphen)
âŒ my..bucket (consecutive dots)
âŒ -mybucket (starts with hyphen)
```

### Real-World Bucket Strategy

```
Company: TechCorp
Buckets needed:

Production:
â”œâ”€ techcorp-prod-data
â”œâ”€ techcorp-prod-logs
â””â”€ techcorp-prod-backups

Development:
â”œâ”€ techcorp-dev-data
â”œâ”€ techcorp-dev-logs
â””â”€ techcorp-dev-backups

Archives:
â””â”€ techcorp-archives-2024

Total: 8 buckets for complete isolation
```

### Cost of S3

```
Storage: $0.023 per GB/month (first 50 TB)
Example: 100 GB/month = $2.30/month

PUT Requests: $0.005 per 1,000 requests
GET Requests: $0.0004 per 1,000 requests
Data Transfer Out: $0.09 per GB

Free tier: 5 GB storage + 20,000 GET/PUT requests/month
(Good for initial labs!)
```

### Validation Checklist
- [ ] Bucket created with unique name
- [ ] Region selected (us-east-1)
- [ ] Versioning enabled
- [ ] Encryption enabled
- [ ] Block Public Access enabled
- [ ] Can see bucket in S3 console
- [ ] Understand bucket is empty

### Key Takeaway
âœ… S3 buckets store objects  
âœ… Names must be globally unique  
âœ… Versioning enables recovery  
âœ… Encryption enabled by default (new buckets)  

---

### Lab 2: Uploading, Downloading & Managing Objects

**Objective:** Upload files to S3 and practice basic operations.

**Why This Matters:**
- Objects = Files in S3
- Can upload/download from console or CLI
- Need to understand object metadata
- Practice with real files

### Understanding Objects

```
Object = File + Metadata

File: document.pdf
Metadata:
â”œâ”€ Key (path): documents/2024/document.pdf
â”œâ”€ Size: 2.5 MB
â”œâ”€ Last Modified: 2024-01-15
â”œâ”€ Storage Class: Standard
â”œâ”€ Encryption: AES-256
â”œâ”€ ETag (version hash): a1b2c3d4...
â””â”€ Tags: department=finance, year=2024
```

### Step-By-Step Instructions

**Step 1: Create Test Files**

On your computer, create test files:

```bash
# Create a text file
echo "This is a test document" > test-document.txt

# Create a CSV file
cat > data.csv << EOF
Name,Age,City
Alice,30,New York
Bob,25,Los Angeles
Charlie,35,Chicago
EOF

# Create a JSON file
cat > config.json << EOF
{
  "app": "MyApp",
  "version": "1.0",
  "environment": "production"
}
EOF

# Create a larger file for testing
dd if=/dev/zero of=large-file.bin bs=1M count=10
# Creates 10 MB file
```

**Step 2: Upload via Console (Simple)**

- Go to S3 bucket
- Click "Upload" button
- Click "Add files"
- Select: test-document.txt
- Click "Upload"

Watch progress bar â†’ File appears in bucket!

**Step 3: Upload Multiple Files**

- Click "Upload"
- Select: data.csv, config.json, large-file.bin
- Click "Upload"
- All files uploaded!

**Step 4: View Object Properties**

- Click on test-document.txt
- See properties:
  - Key (path)
  - Size
  - Storage class
  - Last modified
  - ETag
  - Encryption method

**Step 5: Add Object Tags**

- Click on object
- Go to "Tags"
- Add tags:
  - Key: "type", Value: "document"
  - Key: "backup", Value: "true"
  - Click "Save"

Tags help organize and manage objects!

**Step 6: Download from Console**

- Select test-document.txt
- Click "Download"
- File downloads to your computer
- Verify: Open file, see content

**Step 7: Upload via AWS CLI (Powerful)**

```bash
# Configure AWS CLI (one-time)
aws configure
# Enter: Access Key, Secret Key, Region, Output format

# Upload single file
aws s3 cp test-document.txt s3://my-first-s3-bucket-xxx/

# Upload with options
aws s3 cp data.csv s3://my-first-s3-bucket-xxx/2024-data.csv \
  --storage-class GLACIER \
  --metadata department=finance,backup=true

# Upload entire folder
aws s3 cp ./documents s3://my-first-s3-bucket-xxx/documents/ --recursive

# Upload with progress
aws s3 cp large-file.bin s3://my-first-s3-bucket-xxx/ --metadata source=local

# List all objects
aws s3 ls s3://my-first-s3-bucket-xxx/ --recursive
```

**Step 8: Download via CLI**

```bash
# Download single object
aws s3 cp s3://my-first-s3-bucket-xxx/test-document.txt ./

# Download entire bucket
aws s3 sync s3://my-first-s3-bucket-xxx/ ./my-bucket-backup/

# Download with specific prefix
aws s3 cp s3://my-first-s3-bucket-xxx/documents/ ./docs/ --recursive
```

**Step 9: Copy Between Objects**

```bash
# Copy within same bucket
aws s3 cp s3://my-first-s3-bucket-xxx/test-document.txt \
         s3://my-first-s3-bucket-xxx/backup/test-document.txt

# Copy to another bucket
aws s3 cp s3://my-first-s3-bucket-xxx/test-document.txt \
         s3://another-bucket/

# Copy with metadata
aws s3 cp s3://my-first-s3-bucket-xxx/data.csv \
         s3://my-first-s3-bucket-xxx/archive/data.csv \
         --metadata archived=true,date=2024-01-15
```

**Step 10: Delete Objects**

```bash
# Delete single object
aws s3 rm s3://my-first-s3-bucket-xxx/test-document.txt

# Delete all objects with prefix
aws s3 rm s3://my-first-s3-bucket-xxx/backup/ --recursive

# Delete entire bucket (must be empty first!)
aws s3 rb s3://my-first-s3-bucket-xxx/
```

### S3 Console vs CLI

```
Use Console When:
âœ… Uploading single files
âœ… Quick exploration
âœ… Visual verification
âœ… Modifying metadata

Use CLI When:
âœ… Uploading many files
âœ… Automation/scripting
âœ… Batch operations
âœ… Advanced options (storage class, metadata)
```

### Validation Checklist
- [ ] Test files created locally
- [ ] Files uploaded via console
- [ ] Can view object properties
- [ ] Tags added to objects
- [ ] Downloaded files from console
- [ ] Uploaded files via CLI
- [ ] Listed objects with CLI
- [ ] Downloaded via CLI
- [ ] Objects stored correctly in bucket

### Key Takeaway
âœ… Objects = Files with metadata  
âœ… Upload via console or CLI  
âœ… Tags organize objects  
âœ… Can download/copy/delete anytime  

---

### Lab 3: Access Control & Permissions

**Objective:** Control who can access your S3 bucket and objects.

**Why This Matters:**
- S3 is private by default (secure)
- Need to grant access for users/apps
- Different permission levels available
- Critical for security

### Understanding S3 Access Control

```
Who can access S3?

1. Bucket Owner (Account Owner)
   â””â”€ Full access by default

2. IAM Users in Same Account
   â”œâ”€ Need bucket policy or IAM policy
   â””â”€ Fine-grained permissions

3. Other AWS Accounts
   â”œâ”€ Need bucket policy with Principal
   â””â”€ Cross-account access

4. Public (Internet)
   â”œâ”€ Need explicit public permissions
   â”œâ”€ Usually NOT recommended
   â””â”€ Except for static websites

Access Methods:
â”œâ”€ Bucket Policy (resource-based)
â”œâ”€ IAM Policy (identity-based)
â”œâ”€ ACL (legacy, not recommended)
â”œâ”€ Pre-signed URLs (temporary access)
â””â”€ Bucket Public Access
```

### Step-By-Step Instructions

**Step 1: Create IAM User for Testing**

- Go to IAM Console
- Click "Users"
- Click "Create User"
- Name: `s3-test-user`
- Click "Create User"

**Step 2: Create Access Keys for User**

- Select user
- Go to "Security Credentials"
- Click "Create Access Key"
- Choose: "Command Line Interface (CLI)"
- Click "Create Access Key"
- Download CSV (save safely!)

**Step 3: Attach S3 Policy to User**

Option A: Full S3 Access (for testing)
- Go to user
- Click "Add Permissions"
- Attach policy: `AmazonS3FullAccess`
- Click "Add Permissions"

Option B: Specific Bucket Access (better)
- Create custom policy:

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": "s3:*",
      "Resource": [
        "arn:aws:s3:::my-first-s3-bucket-xxx",
        "arn:aws:s3:::my-first-s3-bucket-xxx/*"
      ]
    }
  ]
}
```

**Step 4: Create Bucket Policy for Public Access**

- Go to bucket â†’ Permissions
- Click "Bucket Policy"
- Add policy:

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "PublicRead",
      "Effect": "Allow",
      "Principal": "*",
      "Action": [
        "s3:GetObject"
      ],
      "Resource": [
        "arn:aws:s3:::my-first-s3-bucket-xxx/public/*"
      ]
    },
    {
      "Sid": "AllowAccountAccess",
      "Effect": "Allow",
      "Principal": {
        "AWS": "arn:aws:iam::123456789012:user/s3-test-user"
      },
      "Action": "s3:*",
      "Resource": [
        "arn:aws:s3:::my-first-s3-bucket-xxx",
        "arn:aws:s3:::my-first-s3-bucket-xxx/*"
      ]
    }
  ]
}
```

**Step 5: Test User Access**

Using s3-test-user credentials:
```bash
# Configure CLI with new user
aws configure --profile s3-test-user
# Enter: Access Key, Secret Key

# List bucket contents
aws s3 ls s3://my-first-s3-bucket-xxx/ --profile s3-test-user
# Should work!

# Upload file
aws s3 cp test.txt s3://my-first-s3-bucket-xxx/ --profile s3-test-user
# Should work!

# Try to access another bucket (should fail)
aws s3 ls s3://different-bucket/ --profile s3-test-user
# Should get Access Denied âœ…
```

**Step 6: Create Pre-signed URL (Temporary Access)**

```bash
# Generate URL valid for 1 hour
aws s3 presign s3://my-first-s3-bucket-xxx/test-document.txt \
  --expires-in 3600

# Output:
# https://my-first-s3-bucket-xxx.s3.amazonaws.com/test-document.txt?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=...

# Anyone with this URL can download (for 1 hour)
# After 1 hour, access denied
```

**Step 7: Test Public Access**

Upload file to public prefix:
```bash
aws s3 cp public-file.txt s3://my-first-s3-bucket-xxx/public/
```

Generate public URL:
```
https://my-first-s3-bucket-xxx.s3.amazonaws.com/public/public-file.txt
```

Try in browser:
- With bucket policy allowing public: File downloads âœ…
- Without policy: Access Denied âŒ

### Permission Hierarchy

```
Most Restrictive (Most Secure)
â”‚
â”œâ”€ Block Public Access: ON
â”‚  â””â”€ Nobody from internet can access
â”‚
â”œâ”€ Bucket Policy: Explicit Deny
â”‚  â””â”€ Cannot be overridden (even by Allow)
â”‚
â”œâ”€ No Permissions
â”‚  â””â”€ Cannot access
â”‚
â”œâ”€ IAM Policy: Allow
â”‚  â””â”€ Can access (within account)
â”‚
â”œâ”€ Bucket Policy: Allow
â”‚  â””â”€ Can access (may be from any account)
â”‚
â””â”€ Public Access: Allow
   â””â”€ Anyone from internet can access

Least Restrictive (Least Secure)
```

### Real-World Scenarios

```
Scenario 1: Website Static Files
â”œâ”€ Bucket Policy: Allow public read to /website/*
â”œâ”€ IAM User: Allow write for CI/CD
â””â”€ Result: Users read, admins update

Scenario 2: Backup Bucket
â”œâ”€ Block Public Access: All ON
â”œâ”€ IAM Users: Limited to put/get
â”œâ”€ Another Account: Deny access
â””â”€ Result: Locked down backup

Scenario 3: Shared Data
â”œâ”€ Bucket Policy: Allow specific accounts
â”œâ”€ Pre-signed URLs: Time-limited access
â””â”€ Result: Secure sharing
```

### Validation Checklist
- [ ] IAM user created
- [ ] Access keys generated
- [ ] Bucket policy created
- [ ] User can access bucket via CLI
- [ ] User cannot access other buckets
- [ ] Pre-signed URL works
- [ ] Public files accessible via URL
- [ ] Private files require authentication

### Key Takeaway
âœ… S3 is private by default  
âœ… Bucket policies control access  
âœ… IAM policies control users  
âœ… Pre-signed URLs = temporary access  

---

### Lab 4: Storage Classes & Lifecycle Policies

**Objective:** Optimize costs by moving objects to cheaper storage.

**Why This Matters:**
- Different storage classes for different needs
- Hot data (frequent access) = expensive
- Cold data (rare access) = cheap
- Lifecycle automatically moves data

### Understanding Storage Classes

```
Storage Class      Cost/GB/Month  Access Speed  Best For
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
STANDARD           $0.023         Immediate    Active data
STANDARD_IA        $0.0125        Immediate    Infrequent access
INTELLIGENT_TIERING $0.020        Immediate    Unknown patterns
GLACIER            $0.004         Hours        Archives
DEEP_ARCHIVE       $0.00099       Days         Long-term backup

Cost comparison for 100 GB:
STANDARD:    100 Ã— $0.023 = $2.30/month
GLACIER:     100 Ã— $0.004 = $0.40/month
DEEP_ARCHIVE: 100 Ã— $0.00099 = $0.099/month

Savings: 95%+ for cold data! ğŸ’°
```

### Step-By-Step Instructions

**Step 1: Understand Current Storage Class**

- Click on object
- Properties â†’ Storage Class
- Shows: STANDARD (default)
- Can change manually for testing

**Step 2: Create Lifecycle Policy**

- Go to bucket â†’ Lifecycle
- Click "Create Lifecycle Rule"
- Name: `OptimizeCosts`

**Step 3: Configure Transitions**

Rule Actions:
```
Transition objects:
â”œâ”€ After 30 days â†’ STANDARD_IA
â”œâ”€ After 90 days â†’ GLACIER
â””â”€ After 365 days â†’ DEEP_ARCHIVE
```

In console:
- Under "Transition" section
- Add transition:
  - Days: 30
  - Storage Class: STANDARD_IA
- Add transition:
  - Days: 90
  - Storage Class: GLACIER
- Add transition:
  - Days: 365
  - Storage Class: DEEP_ARCHIVE

**Step 4: Configure Expiration**

Delete very old objects:
- After 2555 days (7 years)
- Delete object (permanent)

```
Transition Timeline:
Day 0: File uploaded (STANDARD, costs $0.023/GB)
Day 30: Auto moved to STANDARD_IA (costs $0.0125/GB)
Day 90: Auto moved to GLACIER (costs $0.004/GB)
Day 365: Auto moved to DEEP_ARCHIVE (costs $0.00099/GB)
Day 2555: Auto deleted
```

**Step 5: Apply to Prefix (Not All)**

Instead of all objects, target specific:
- Filter â†’ Prefix: `logs/`
- Only objects in "logs/" folder get policy

**Step 6: Enable Versioning for Recovery**

Before lifecycle, enable:
- Go to bucket â†’ Versioning
- Enable versioning
- Now old versions also follow lifecycle!

**Step 7: View Lifecycle Policy**

```json
{
  "Rules": [
    {
      "Id": "OptimizeCosts",
      "Status": "Enabled",
      "Filter": {
        "Prefix": ""
      },
      "Transitions": [
        {
          "Days": 30,
          "StorageClass": "STANDARD_IA"
        },
        {
          "Days": 90,
          "StorageClass": "GLACIER"
        },
        {
          "Days": 365,
          "StorageClass": "DEEP_ARCHIVE"
        }
      ],
      "Expiration": {
        "Days": 2555
      }
    }
  ]
}
```

**Step 8: Test Lifecycle (Manually)**

Real lifecycle takes days, but test:
```bash
# Upload file and note current storage class
aws s3 cp file.txt s3://my-first-s3-bucket-xxx/

# View object metadata
aws s3api head-object \
  --bucket my-first-s3-bucket-xxx \
  --key file.txt \
  --query 'StorageClass'
# Shows: STANDARD

# Manually change storage class (for testing)
aws s3api copy-object \
  --copy-source my-first-s3-bucket-xxx/file.txt \
  --bucket my-first-s3-bucket-xxx \
  --key file.txt \
  --storage-class GLACIER

# Verify
aws s3api head-object \
  --bucket my-first-s3-bucket-xxx \
  --key file.txt \
  --query 'StorageClass'
# Shows: GLACIER
```

### Cost Analysis Example

```
Company Backup Strategy:

100 GB uploaded per day
30-day retention required
365-day optional retention

Monthly costs:

Without Lifecycle:
100 GB Ã— 30 days Ã— $0.023 = $69/month

With Lifecycle:
Days 0-30: STANDARD â†’ $69/month
Days 31-365: GLACIER â†’ $100 GB Ã— 335 days Ã— $0.004 / 30 = $4.47/month
Total: $73.47 first month, then $4.47/month

Annual savings: (~$69-4.47) Ã— 11 months = $710/year! ğŸ’°
```

### Validation Checklist
- [ ] Understand different storage classes
- [ ] Lifecycle policy created
- [ ] Transitions configured
- [ ] Applied to specific prefix
- [ ] Versioning enabled
- [ ] Can view lifecycle rule
- [ ] Understand cost savings

### Key Takeaway
âœ… Storage classes = Different costs  
âœ… Lifecycle auto-transitions objects  
âœ… 95%+ cost reduction possible  
âœ… Set and forget!  

---

## POST-LAB 1-4 CLEANUP

**Stop but keep for later labs:**

```bash
# DO NOT DELETE yet - we'll use these in next labs
# Just keep the bucket and objects

# To check current costs
aws s3 ls --recursive s3://my-first-s3-bucket-xxx/
# Shows all objects and their sizes

# Estimated cost so far:
# Storage: Minimal (free tier covers 5 GB)
# Requests: Free tier covers 20,000 GET/PUT
# Total cost: $0 âœ…
```

---

## SECTION 2: ADVANCED FEATURES (Labs 5-8)

### Lab 5: Versioning & Object Recovery

**Objective:** Recover deleted or modified objects using versions.

**Why This Matters:**
- Accidental deletions happen
- Need to recover old versions
- Versioning enabled = Protection
- MFA delete for extra security

### Understanding Versioning

```
File: document.txt

Without Versioning:
Upload v1.0 â†’ Upload v2.0 (overwrites) â†’ Cannot get v1.0 âŒ

With Versioning:
Upload v1.0 â†’ Version ID: abc123
Upload v2.0 â†’ Version ID: def456
Upload v3.0 â†’ Version ID: ghi789

Now can access ANY version! âœ…
```

### Step-By-Step Instructions

**Step 1: Enable Versioning**

Already enabled in Lab 1! Verify:
- Go to bucket â†’ Properties
- Versioning â†’ Enabled âœ“

**Step 2: Upload Multiple Versions**

```bash
# Upload version 1
echo "Version 1" > document.txt
aws s3 cp document.txt s3://my-first-s3-bucket-xxx/

# Upload version 2 (same name, overwrites)
echo "Version 2 - Updated" > document.txt
aws s3 cp document.txt s3://my-first-s3-bucket-xxx/

# Upload version 3
echo "Version 3 - Final" > document.txt
aws s3 cp document.txt s3://my-first-s3-bucket-xxx/
```

**Step 3: View All Versions**

In console:
- Click on object
- See "Versions" tab
- Shows all 3 versions with Version IDs and timestamps

Via CLI:
```bash
# List all versions
aws s3api list-object-versions \
  --bucket my-first-s3-bucket-xxx \
  --prefix document.txt

# Output shows:
# VersionId: abc123 (v1.0)
# VersionId: def456 (v2.0)
# VersionId: ghi789 (v3.0, current)
```

**Step 4: Download Specific Version**

```bash
# Download current version (v3.0)
aws s3api get-object \
  --bucket my-first-s3-bucket-xxx \
  --key document.txt \
  document-current.txt

cat document-current.txt
# Shows: Version 3 - Final

# Download specific version (v1.0)
aws s3api get-object \
  --bucket my-first-s3-bucket-xxx \
  --key document.txt \
  --version-id abc123 \
  document-v1.txt

cat document-v1.txt
# Shows: Version 1
```

**Step 5: Delete Current Version (Soft Delete)**

```bash
# Delete latest version
aws s3api delete-object \
  --bucket my-first-s3-bucket-xxx \
  --key document.txt

# Check what's current now
aws s3api get-object \
  --bucket my-first-s3-bucket-xxx \
  --key document.txt \
  document-recovered.txt

cat document-recovered.txt
# Shows: Version 2 - Updated (previous version now current!)

# Delete marker shows file is deleted, but versions exist
```

**Step 6: Permanently Delete Version**

```bash
# Delete specific version (PERMANENT)
aws s3api delete-object \
  --bucket my-first-s3-bucket-xxx \
  --key document.txt \
  --version-id abc123

# Version abc123 is now gone forever
```

**Step 7: Enable MFA Delete (Extra Security)**

For root account only:
```bash
# First, enable versioning on bucket (done)

# Then enable MFA delete via AWS CLI
# (Requires MFA device and AWS root credentials)

aws s3api put-bucket-versioning \
  --bucket my-first-s3-bucket-xxx \
  --versioning-configuration \
  'Status=Enabled,MFADelete=Enabled' \
  --mfa 'device-serial 123456'

# Now MUST use MFA to delete any version
# Extra protection! ğŸ”’
```

### Versioning Costs

```
Without Versioning:
100 GB storage = $2.30/month

With Versioning (keeping 5 versions):
100 GB Ã— 5 = 500 GB = $11.50/month
(Extra cost for history)

Trade-off: Pay more for protection & recoverability
For critical data: Worth it!
```

### Real-World Scenario

```
Situation: Database backup corrupted
â”œâ”€ Latest backup (v3.0): Corrupted
â”œâ”€ Previous backup (v2.0): Good
â”œâ”€ Action: Download v2.0
â””â”€ Restore from v2.0

Without versioning: Data lost forever âŒ
With versioning: Recovered in minutes âœ…
```

### Validation Checklist
- [ ] Versioning enabled
- [ ] Multiple versions uploaded
- [ ] Can list all versions
- [ ] Can download specific version
- [ ] Can recover previous version
- [ ] Understand version costs

### Key Takeaway
âœ… Versioning = Protection  
âœ… Recover any previous version  
âœ… MFA delete for critical buckets  
âœ… Trade off: Cost for safety  

---

### Lab 6: Encryption & Security

**Objective:** Secure your S3 data with encryption and access controls.

**Why This Matters:**
- Data at rest encryption
- Data in transit encryption
- Keys managed by AWS or you
- Required for compliance

### Understanding S3 Encryption

```
Encryption Types:

1. Server-Side Encryption (SSE)
   â”œâ”€ SSE-S3: AWS managed keys (FREE, default)
   â”œâ”€ SSE-KMS: Customer master keys (Paid)
   â””â”€ SSE-C: Customer provided keys (you manage)

2. Client-Side Encryption
   â””â”€ Encrypt before uploading (your app)

Flow:
Plain File â†’ S3 â†’ Encrypted at rest
         (HTTPS encrypted in transit)
```

### Step-By-Step Instructions

**Step 1: Enable Default Encryption (Already Done)**

Most new buckets have AES-256 by default:
- Go to bucket â†’ Properties
- Edit "Default Encryption"
- Should show: AES-256 enabled âœ“

**Step 2: Enable S3 Default Encryption for All**

- Still in bucket properties
- Default Encryption:
  - Type: Server-side encryption
  - Encryption Key Management: Amazon S3-managed keys (SSE-S3)
  - Save

**Step 3: Upload Encrypted Object**

```bash
# Upload with encryption (uses default)
aws s3 cp sensitive-data.txt s3://my-first-s3-bucket-xxx/

# Upload with specific encryption
aws s3 cp sensitive-data.txt s3://my-first-s3-bucket-xxx/ \
  --sse AES256

# Verify encryption
aws s3api head-object \
  --bucket my-first-s3-bucket-xxx \
  --key sensitive-data.txt \
  --query 'ServerSideEncryption'
# Shows: AES256 âœ“
```

### Strategy 2: Use Intelligent-Tiering

```
Intelligent-Tiering:
â”œâ”€ Auto-moves between tiers based on access
â”œâ”€ No manual configuration
â”œâ”€ Tier Fee: $0.0025 per 1,000 objects
â”œâ”€ Good for unknown patterns

Cost:
100 GB with 10,000 objects:
â”œâ”€ Normal lifecycle: $2.30
â”œâ”€ Intelligent-tiering: $2.30 (storage) + $0.025 (tier fee) = $2.33
â””â”€ Nearly same, plus automatic optimization!
```

**Step 3: Delete Old Data Automatically**

Lifecycle with expiration:
```bash
# Update lifecycle policy
aws s3api put-bucket-lifecycle-configuration \
  --bucket my-first-s3-bucket-xxx \
  --lifecycle-configuration '{
    "Rules": [
      {
        "Id": "DeleteOldData",
        "Status": "Enabled",
        "Prefix": "logs/",
        "Expiration": {
          "Days": 90
        }
      }
    ]
  }'

# After 90 days, logs/ objects deleted automatically
# Cost: $0 (no storage)
```

**Step 4: Use S3 Intelligent-Tiering**

```bash
# Upload to Intelligent-Tiering instead
aws s3 cp data.txt s3://my-first-s3-bucket-xxx/ \
  --storage-class INTELLIGENT_TIERING

# S3 automatically moves between tiers:
# 0-30 days: Frequent Access (high cost)
# 30-90 days: Infrequent Access (medium cost)
# 90+ days: Archive (low cost)
# Auto-moves based on actual access patterns
```

**Step 5: Reduce Data Transfer Costs**

Use CloudFront instead of direct S3:
```
Direct S3 transfer OUT: $0.09/GB
CloudFront transfer OUT: $0.085/GB

With high volume:
â”œâ”€ 1 TB/month direct: $90
â”œâ”€ 1 TB/month CloudFront: $85
â””â”€ Savings: $5/month + 10x faster!
```

**Step 6: Use S3 Gateway Endpoint**

For EC2 accessing S3 (same region):
```bash
# Create VPC endpoint
aws ec2 create-vpc-endpoint \
  --vpc-id vpc-xxx \
  --service-name com.amazonaws.us-east-1.s3

# Now EC2 â†’ S3 transfer = FREE!
# Instead of: $0.02/GB (inter-region)
# Save: 100%! ğŸ’°
```

**Step 7: Consolidate into Fewer Buckets**

```
Before:
â”œâ”€ Bucket 1: $10/month
â”œâ”€ Bucket 2: $8/month
â”œâ”€ Bucket 3: $7/month
â””â”€ Total: $25/month

After (consolidate):
â””â”€ Bucket 1: $23/month

Savings: $2/month (8%)
Benefit: Simpler management
```

**Step 8: Enable S3 Batch Operations (For Old Data)**

```bash
# Find all STANDARD objects older than 1 year
# Convert to GLACIER in batch

aws s3api create-job \
  --account-id 123456789012 \
  --operation LambdaInvoke \
  --manifest Spec='ETag=xxx',Format=S3BatchOperations_CSV_20180820 \
  --report Spec='reportScope=All' \
  --priority 0 \
  --role-arn arn:aws:iam::123456789012:role/S3BatchRole \
  --tags Key=Purpose,Value=ArchiveOldData

# Processes thousands of objects in parallel
# Convert 1 TB in hours (not days!)
```

### Cost Optimization Summary

```
Strategy              Savings    Effort   Recommendation
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Lifecycle Policy      60-80%     Easy     Start here!
Intelligent-Tiering  50-70%     Easy     Maintenance-free
Delete old data      20-40%     Easy     Regular cleanup
CloudFront           5-15%      Medium   For public data
VPC Endpoint         100%*      Medium   For EC2 in VPC
Consolidate buckets  5-10%      Hard     Reorganize

* For inter-region transfers
Total possible: 80%+ reduction! ğŸ¯
```

### Real-World Cost Reduction

```
Company: DataCorp
Current S3 bill: $2,000/month

Problems identified:
â”œâ”€ All objects in STANDARD (most expensive)
â”œâ”€ Logs never deleted
â”œâ”€ No CloudFront (high transfer costs)
â””â”€ Multiple buckets (inefficient)

Implemented:
1. Lifecycle (30â†’IA, 90â†’GLACIER, delete 2y)
   â””â”€ Savings: $1,200/month
2. Delete logs after 90 days
   â””â”€ Savings: $300/month
3. CloudFront for public data
   â””â”€ Savings: $150/month
4. Consolidate buckets
   â””â”€ Savings: $50/month

New bill: $300/month (85% reduction!) ğŸ’°
```

### Validation Checklist
- [ ] Analyzed current S3 costs
- [ ] Implemented lifecycle policy
- [ ] Enabled Intelligent-Tiering
- [ ] Configured CloudFront
- [ ] Understand data transfer costs
- [ ] Calculate potential savings

### Key Takeaway
âœ… Lifecycle = Biggest savings (60-80%)  
âœ… Delete old data = Quick wins  
âœ… CloudFront = Cost + speed  
âœ… 80%+ reduction possible  

---

## POST-LAB 9-12 CLEANUP

**Begin cleanup (costs accumulating):**

```bash
# Keep one bucket for testing, delete others

# List all buckets
aws s3 ls

# Empty bucket (delete all objects)
aws s3 rm s3://my-first-s3-bucket-xxx/ --recursive

# Or delete backup bucket (if created)
aws s3 rm s3://my-first-s3-bucket-backup-xxx/ --recursive

# Current estimated costs:
# Storage: $0-5 (if objects remain)
# CloudFront: $0 (if low traffic)
# Total: Minimal
```

---

## SECTION 4: ADVANCED & COMPLIANCE (Labs 13-16)

### Lab 13: Cross-Account Access & Bucket Policies

**Objective:** Share S3 bucket with another AWS account.

**Why This Matters:**
- Multi-team collaboration
- Third-party integration
- Secure cross-account access
- Audit trail

### Step-By-Step Instructions

**Step 1: Create Another AWS Account (Or Use Existing)**

For testing, we'll simulate different account.
Note the account ID: `123456789012` (source)
Other account ID: `210987654321` (destination)

**Step 2: Create Bucket Policy for Cross-Account**

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "AllowCrossAccountAccess",
      "Effect": "Allow",
      "Principal": {
        "AWS": "arn:aws:iam::210987654321:root"
      },
      "Action": [
        "s3:GetObject",
        "s3:ListBucket"
      ],
      "Resource": [
        "arn:aws:s3:::my-first-s3-bucket-xxx",
        "arn:aws:s3:::my-first-s3-bucket-xxx/*"
      ]
    }
  ]
}
```

Add to bucket policy:
- Go to bucket â†’ Permissions â†’ Bucket Policy
- Add above statement
- Save

**Step 3: Create IAM Role in Source Account**

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:GetObject",
        "s3:ListBucket"
      ],
      "Resource": [
        "arn:aws:s3:::my-first-s3-bucket-xxx",
        "arn:aws:s3:::my-first-s3-bucket-xxx/*"
      ]
    }
  ]
}
```

Create role: `CrossAccountS3Access`

**Step 4: Test Cross-Account Access**

From destination account:
```bash
# Assume role
aws sts assume-role \
  --role-arn arn:aws:iam::123456789012:role/CrossAccountS3Access \
  --role-session-name test-session

# Use returned credentials
export AWS_ACCESS_KEY_ID=...
export AWS_SECRET_ACCESS_KEY=...
export AWS_SESSION_TOKEN=...

# List bucket in source account
aws s3 ls s3://my-first-s3-bucket-xxx/
# Works! âœ…
```

### Real-World Scenario

```
Company with multiple departments:

Engineering Account:
â”œâ”€ Bucket: engineering-code

Marketing Account:
â”œâ”€ Wants to: Access engineering assets
â”œâ”€ Gets: Cross-account role
â””â”€ Access: Limited to /public/* folder

Finance Account:
â”œâ”€ Wants to: Backup all departments
â”œâ”€ Gets: Cross-account role
â””â”€ Access: Write to /backups/

Engineering retains full control,
others get only what they need! âœ…
```

### Validation Checklist
- [ ] Understand cross-account access
- [ ] Bucket policy allows other account
- [ ] IAM role created
- [ ] Can assume role from other account
- [ ] Can access S3 with cross-account permissions

### Key Takeaway
âœ… Cross-account access possible  
âœ… Bucket policies enable sharing  
âœ… Audit trail maintained  
âœ… Fine-grained control  

---

### Lab 14: S3 Object Lock & Compliance

**Objective:** Make objects immutable for compliance.

**Why This Matters:**
- WORM (Write Once Read Many)
- Regulatory compliance
- Prevent accidental deletion
- Audit requirements

### Understanding Object Lock

```
Object Lock Modes:

Governance Mode:
â”œâ”€ Can DELETE with special permission
â”œâ”€ Can EXTEND retention
â””â”€ For most use cases

Compliance Mode:
â”œâ”€ Cannot DELETE even with admin
â”œâ”€ Cannot SHORTEN retention
â”œâ”€ Cannot MODIFY metadata
â””â”€ For strict compliance (SEC, HIPAA)
```

### Step-By-Step Instructions

**Step 1: Enable Object Lock (During Creation)**

Object Lock must be enabled at bucket creation.
We enabled it in Lab 6!

Verify:
- Go to bucket â†’ Properties
- Object Lock Status: Enabled âœ“

**Step 2: Upload Object with Retention**

```bash
# Upload with Governance retention
aws s3api put-object \
  --bucket my-first-s3-bucket-xxx \
  --key compliance-data.txt \
  --body compliance-data.txt \
  --object-lock-mode GOVERNANCE \
  --object-lock-retain-until-date 2024-12-31T00:00:00Z

# Object cannot be deleted until 2024-12-31
```

**Step 3: Upload with Legal Hold**

```bash
# Legal hold = Indefinite protection
aws s3api put-object \
  --bucket my-first-s3-bucket-xxx \
  --key evidence.txt \
  --body evidence.txt \
  --object-lock-legal-hold-status ON

# Cannot delete until legal hold removed
# Even after retention expires!
```

**Step 4: Try to Delete (Should Fail)**

```bash
# Try to delete file with retention
aws s3 rm s3://my-first-s3-bucket-xxx/compliance-data.txt

# Error: Object Lock Governance mode
# retention period has not expired

# Try to modify metadata
aws s3api put-object-tagging \
  --bucket my-first-s3-bucket-xxx \
  --key compliance-data.txt \
  --tagging 'TagSet=[{Key=archive,Value=true}]'

# Error: Cannot modify locked object
```

**Step 5: Extend Retention Period**

```bash
# Admin can extend (not shorten)
aws s3api put-object-retention \
  --bucket my-first-s3-bucket-xxx \
  --key compliance-data.txt \
  --retention 'Mode=GOVERNANCE,RetainUntilDate=2025-12-31T00:00:00Z' \
  --bypass-governance-retention

# --bypass-governance-retention: Admin override
# --object-lock-token: Required for compliance mode
```

### Real-World Compliance

```
Company: FinanceCorp
Regulatory requirement: Keep financial records 7 years

Solution: S3 with Object Lock
â”œâ”€ Bucket with Compliance mode enabled
â”œâ”€ Upload all financial records
â”œâ”€ Retention: 7 years (2555 days)
â”œâ”€ No one can delete before 7 years
â””â”€ Perfect compliance! âœ…

Benefits:
â”œâ”€ No accidental deletion
â”œâ”€ Audit trail (who tried to delete)
â”œâ”€ Regulatory audit proof
â””â”€ Peace of mind
```

### Validation Checklist
- [ ] Object Lock enabled
- [ ] Upload object with retention
- [ ] Upload object with legal hold
- [ ] Cannot delete locked object
- [ ] Can extend retention (if admin)
- [ ] Understand governance vs compliance

### Key Takeaway
âœ… Object Lock = Immutability  
âœ… Governance = Admin override possible  
âœ… Compliance = True WORM  
âœ… Required for regulations  

---

### Lab 15: S3 Access Points & Multi-Region

**Objective:** Simplify access and enable multi-region.

**Why This Matters:**
- Simplified access management
- Multiple regions easily
- Consistent policy application
- Scalable architecture

### Understanding Access Points

```
Traditional: Application â†’ S3 Bucket
â””â”€ One endpoint, one policy

With Access Points:
Application A â†’ AP1 (different policy)
Application B â†’ AP2 (different policy)
All point to same bucket!

Benefits:
â”œâ”€ Separate endpoints per app
â”œâ”€ Independent policies
â”œâ”€ Consistent auditing
â””â”€ Easy multi-region
```

### Step-By-Step Instructions

**Step 1: Create Access Point**

- Go to bucket â†’ Access Points
- Click "Create Access Point"
- Name: `AppAccessPoint`
- Network origin: Internet (for now)
- Block Public Access: OFF (if you want public)
- Create

**Step 2: Get Access Point ARN**

```
arn:aws:s3:us-east-1:123456789012:accesspoint/AppAccessPoint
```

**Step 3: Create Access Point Policy**

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Principal": "*",
      "Effect": "Allow",
      "Action": ["s3:GetObject"],
      "Resource": ["arn:aws:s3:us-east-1:123456789012:accesspoint/AppAccessPoint/object/*"]
    }
  ]
}
```

**Step 4: Use Access Point for Access**

```bash
# Access via access point
aws s3api get-object \
  --bucket arn:aws:s3:us-east-1:123456789012:accesspoint/AppAccessPoint \
  --key file.txt \
  output.txt

# Works just like bucket! âœ“
```

**Step 5: Create Multi-Region Access Point (Optional)**

For truly global:
- Go to Multi-Region Access Points
- Create MRAP
- Add access points from multiple regions
- Get MRAP endpoint: `mfzwi23gnjvgw.mrap.accesspoint.s3.amazonaws.com`

Use single endpoint for all regions!

### Real-World Architecture

```
Company: GlobalTech
Offices in: US, EU, Asia

Architecture:
US Bucket
â”œâ”€ Access Point: us-app
â””â”€ Replicated to eu-west-1

EU Bucket
â””â”€ Access Point: eu-app

Asia Bucket
â””â”€ Access Point: ap-app

Multi-Region AP:
â””â”€ Routes to nearest region

Users in Tokyo â†’ MRAP â†’ ap-app (fast)
Users in London â†’ MRAP â†’ eu-app (fast)
Users in NYC â†’ MRAP â†’ us-app (fast)

Perfect performance globally! ğŸŒ
```

### Validation Checklist
- [ ] Access Point created
- [ ] Can access objects via AP
- [ ] Understand AP benefits
- [ ] Know MRAP for multi-region

### Key Takeaway
âœ… Access Points simplify access  
âœ… Independent policies per AP  
âœ… MRAP for global applications  
âœ… Easy multi-region  

---

### Lab 16: S3 Batch Operations & Advanced Automation

**Objective:** Perform large-scale operations automatically.

**Why This Matters:**
- Modify millions of objects
- Change storage class at scale
- Tag all objects
- Copy between buckets
- Automated workflows

### Understanding Batch Operations

```
Without Batch Operations:
Modify 1 million objects:
â””â”€ Do it manually = Weeks! âŒ

With Batch Operations:
Modify 1 million objects:
â””â”€ Create job = Hours! âœ…
```

### Step-By-Step Instructions

**Step 1: Prepare Manifest File**

Create inventory of objects to process:
```csv
Bucket,Key
my-first-s3-bucket-xxx,file1.txt
my-first-s3-bucket-xxx,file2.txt
my-first-s3-bucket-xxx,folder/file3.txt
```

Upload manifest:
```bash
aws s3 cp manifest.csv s3://my-first-s3-bucket-xxx/manifests/
```

**Step 2: Create Batch Operation Job**

```bash
# Copy objects to different bucket
aws s3api create-job \
  --account-id 123456789012 \
  --operation 'Copy' \
  --manifest '{
    "Spec": {
      "Format": "S3BatchOperations_CSV_20180820"
    },
    "Location": {
      "ObjectArn": "arn:aws:s3:::my-first-s3-bucket-xxx/manifests/manifest.csv",
      "ETag": "abc123"
    }
  }' \
  --report '{
    "Spec": {
      "Format": "Report_CSV_20180820",
      "Enabled": true,
      "Prefix": "s3://my-first-s3-bucket-xxx/reports/",
      "ReportScope": "AllTasks"
    }
  }' \
  --priority 10 \
  --role-arn arn:aws:iam::123456789012:role/S3BatchRole \
  --confirmation-required true \
  --description "Copy all objects to archive bucket"

# Returns Job ID: 123456789abc
```

**Step 3: Monitor Job Progress**

```bash
# Check job status
aws s3api describe-job \
  --account-id 123456789012 \
  --job-id 123456789abc

# Shows:
# â”œâ”€ Status: Preparing (or Running, Complete)
# â”œâ”€ ProgressSummary
# â”‚  â”œâ”€ TotalNumberOfTasks: 3
# â”‚  â”œâ”€ NumberOfTasksSucceeded: 2
# â”‚  â”œâ”€ NumberOfTasksFailed: 0
# â”‚  â””â”€ NumberOfTasksInProgress: 1
# â””â”€ PercentageProgress: 66%
```

**Step 4: Update Job Tags (Tag 1 Million Objects)**

Create job to add tags:
```bash
# All objects get department=finance tag
aws s3api update-job-status \
  --account-id 123456789012 \
  --job-id 123456789abc \
  --status-update-reason "Tagging financial records" \
  --requested-job-status Ready

# Job processes all objects
# Adds tags in parallel
# Much faster than one-by-one! âš¡
```

**Step 5: Change Storage Class (Millions of Objects)**

```bash
# Move everything older than 30 days to GLACIER
# Create manifest of old objects
# Create batch job to change storage class
# All done in hours!

# Manual approach: Days/weeks
# Batch operation approach: Hours
```

### Real-World Scenario

```
Problem: Company acquired another company
New company's S3: 50 million objects
Need to:
â”œâ”€ Move to company's bucket
â”œâ”€ Tag with department
â”œâ”€ Change storage class
â””â”€ Timeline: 1 week

Solution: S3 Batch Operations
â””â”€ Create 3 jobs (copy, tag, storage class)
â””â”€ All run in parallel
â””â”€ Completed in 2 days! âœ…

Cost: ~$5 for job execution
Manual approach: $10,000+ (staff time)
```

### Batch Operations Types

```
Available Operations:

1. Copy (Copy objects between buckets)
2. Invoke Lambda (Custom processing)
3. Put Object Tagging (Add tags)
4. Replace Object Tagging (Update tags)
5. Put Object ACL (Change permissions)
6. Put Object Legal Hold (Add legal hold)
7. Put Object Retention (Set retention)
8. Restore Object (Restore from GLACIER)
9. S3 Object Lock Retention (Update retention)
10. S3 Object Lock Legal Hold (Update hold)
```

### Validation Checklist
- [ ] Created manifest file
- [ ] Created batch operation job
- [ ] Monitored job progress
- [ ] Job completed successfully
- [ ] Can see operation results
- [ ] Understand batch benefits

### Key Takeaway
âœ… Batch Operations = Massive scale  
âœ… Process millions in hours  
âœ… Parallel execution  
âœ… Cost-effective  

---

## COMPLETE CLEANUP PROCEDURE

### Full S3 Cleanup

**Step 1: Empty All Buckets**

```bash
# List buckets
aws s3 ls

# Empty each bucket
aws s3 rm s3://my-first-s3-bucket-xxx/ --recursive
aws s3 rm s3://my-first-s3-bucket-backup-xxx/ --recursive

# If versioning enabled, must delete all versions
aws s3api list-object-versions \
  --bucket my-first-s3-bucket-xxx \
  --query 'Versions[].{Key:Key,VersionId:VersionId}' \
  --output table

# Delete all versions
aws s3api delete-objects \
  --bucket my-first-s3-bucket-xxx \
  --delete 'Objects=[{Key=file.txt,VersionId=abc123}]'
```

**Step 2: Disable/Remove Configurations**

```bash
# Disable versioning
aws s3api put-bucket-versioning \
  --bucket my-first-s3-bucket-xxx \
  --versioning-configuration 'Status=Suspended'

# Delete lifecycle
aws s3api delete-bucket-lifecycle \
  --bucket my-first-s3-bucket-xxx

# Delete replication
aws s3api delete-bucket-replication \
  --bucket my-first-s3-bucket-xxx

# Delete access points
aws s3api delete-access-point \
  --account-id 123456789012 \
  --name AppAccessPoint

# Delete batch operations
# (Already completed, auto-cleaned)

# Disable logging
aws s3api put-bucket-logging \
  --bucket my-first-s3-bucket-xxx \
  --bucket-logging-status '{}'

# Remove bucket policy
aws s3api delete-bucket-policy \
  --bucket my-first-s3-bucket-xxx

# Remove CORS
aws s3api delete-bucket-cors \
  --bucket my-first-s3-bucket-xxx

# Remove website configuration
aws s3api delete-bucket-website \
  --bucket my-first-s3-bucket-xxx
```

**Step 3: Delete CloudFront Distribution**

```bash
# Disable distribution first
aws cloudfront update-distribution-config \
  --id D123ABC \
  --distribution-config file://disabled-config.json

# Wait for disabling (5-10 minutes)

# Delete distribution
aws cloudfront delete-distribution \
  --id D123ABC \
  --if-match ETAG
```

**Step 4: Delete Lambda Function**

```bash
# Delete Lambda function
aws lambda delete-function \
  --function-name S3FileProcessor
```

**Step 5: Delete IAM Roles**

```bash
# List roles
aws iam list-roles

# Delete inline policies first
aws iam delete-role-policy \
  --role-name S3ReplicationRole \
  --policy-name S3ReplicationPolicy

# Then delete role
aws iam delete-role \
  --role-name S3ReplicationRole
```

**Step 6: Delete Buckets (Finally!)**

```bash
# Delete each bucket (must be empty!)
aws s3 rb s3://my-first-s3-bucket-xxx/
aws s3 rb s3://my-first-s3-bucket-backup-xxx/
aws s3 rb s3://my-first-s3-bucket-logging/

# Verify deleted
aws s3 ls
# Should not show your buckets
```

**Step 7: Verify Complete Cleanup**

```bash
# Check S3
aws s3 ls
# Empty (no custom buckets)

# Check CloudFront
aws cloudfront list-distributions
# Empty (no custom distributions)

# Check Lambda
aws lambda list-functions
# Empty (no custom functions)

# Check IAM roles
aws iam list-roles | grep custom
# Empty (no custom roles)

# Check CloudWatch
aws cloudwatch describe-dashboards
# Empty (no custom dashboards)
```

**Step 8: Verify Billing**

AWS Billing Dashboard should show:
- All S3 charges: $0
- All CloudFront charges: $0
- All Lambda charges: $0
- All other services: Minimal

### Cost Summary

```
Total cost for all 16 labs:

Free tier coverage:
â”œâ”€ 5 GB storage: FREE
â”œâ”€ 20,000 GET/PUT: FREE
â”œâ”€ 1M Lambda requests: FREE
â””â”€ CloudFront: Some cost (~$0-5)

Typical cost: $5-20 total
(If you used CloudFront/replication extensively)

Labs without free tier activation:
â”œâ”€ NAT Gateway: ~$2
â”œâ”€ CloudFront: ~$2-5
â”œâ”€ Replication: ~$2
â””â”€ Total: ~$6-10

**Should be well within free tier!**
```

### Cleanup Checklist

- [ ] All buckets emptied
- [ ] All bucket configurations removed
- [ ] CloudFront distribution deleted
- [ ] Lambda function deleted
- [ ] IAM roles deleted
- [ ] All buckets deleted
- [ ] CloudWatch dashboards cleaned
- [ ] AWS Billing shows $0 S3 charges
- [ ] Verified all resources removed

---

## SUMMARY: What You Learned

### Labs 1-4: Fundamentals
- âœ… Create and configure buckets
- âœ… Upload/download objects
- âœ… Access control & permissions
- âœ… Storage classes & lifecycle

### Labs 5-8: Advanced Features
- âœ… Versioning & recovery
- âœ… Encryption & security
- âœ… Static website hosting
- âœ… Event notifications & Lambda

### Labs 9-12: Production Features
- âœ… Replication (backup & disaster recovery)
- âœ… CloudFront integration
- âœ… Monitoring & analytics
- âœ… Cost optimization

### Labs 13-16: Advanced & Compliance
- âœ… Cross-account access
- âœ… Object Lock & compliance
- âœ… Access Points & multi-region
- âœ… Batch Operations & automation

---

## AWS S3 Best Practices

### Security
âœ… Enable versioning (recovery)
âœ… Enable encryption (AES-256 default)
âœ… Block public access by default
âœ… Implement bucket policies carefully
âœ… Enable access logging
âœ… Use Object Lock for compliance
âœ… Rotate credentials regularly

### Cost Optimization
âœ… Implement lifecycle policies (biggest savings!)
âœ… Delete old data regularly
âœ… Use CloudFront for public content
âœ… Use VPC endpoints for EC2
âœ… Consider Intelligent-Tiering for unknown patterns
âœ… Consolidate buckets
âœ… Monitor costs with CloudWatch

### Performance
âœ… Use CloudFront for global access
âœ… Implement multipart upload for large files
âœ… Batch operations for bulk changes
âœ… Access Points for simplified access
âœ… Proper CORS configuration
âœ… Enable compression at CloudFront

### Reliability
âœ… Enable versioning
âœ… Cross-region replication
âœ… Regular backups
âœ… Retention policies
âœ… Multi-region access points
âœ… MFA delete for critical data

---

## Real-World S3 Use Cases

```
1. Data Lake
â”œâ”€ Raw data in STANDARD
â”œâ”€ Processed in STANDARD_IA
â”œâ”€ Archives in GLACIER
â””â”€ Analytics with Athena

2. Media Hosting
â”œâ”€ Videos in S3
â”œâ”€ Distributed via CloudFront
â”œâ”€ Thumbnails auto-generated
â””â”€ Analytics tracked

3. Backup & Disaster Recovery
â”œâ”€ Daily backups to S3
â”œâ”€ Cross-region replication
â”œâ”€ Lifecycle to GLACIER after 30 days
â””â”€ 7-year retention

4. Content Repository
â”œâ”€ Documents in S3
â”œâ”€ Versioning enabled
â”œâ”€ Access controls per folder
â””â”€ Search via Athena

5. Log Storage
â”œâ”€ Application logs
â”œâ”€ CloudFront logs
â”œâ”€ S3 access logs
â”œâ”€ Lifecycle: Delete after 90 days
â””â”€ Analysis with Athena/Splunk
```

---

## Congratulations! ğŸ‰

You've completed 16 comprehensive S3 labs covering:
- Basic bucket operations
- Advanced security
- Cost optimization
- Production-ready features
- Compliance requirements
- Automation & batch operations

You now have enterprise-level S3 knowledge!

**Next steps:**
1. Practice with real data
2. Implement cost optimizations
3. Set up monitoring & alerts
4. Design multi-region strategy
5. Build automated pipelines

Happy S3-ing! ğŸš€"AES256", "aws:kms"]
        }
      }
    }
  ]
}
```

Add to bucket policy:
- Go to bucket â†’ Permissions â†’ Bucket Policy
- Add above statement
- Click "Save"

Test enforcement:
```bash
# Try to upload without encryption (using presigned URL)
# Will be rejected âœ…
```

**Step 5: Enable Block Public Access**

Extra security layer:

- Go to bucket â†’ Permissions
- Block Public Access:
  - All 4 options: ON âœ“
  - Prevents public access even with policy
  - Best practice!

**Step 6: Enable Object Lock (Immutable)**

For compliance requirements:

- Go to bucket (during creation or after):
- Object Lock: Enable
- Mode: Compliance or Governance
  - Compliance: CANNOT override (WORM)
  - Governance: Can override with permission

Once enabled, objects cannot be deleted for retention period!

**Step 7: Enable Access Logging**

Log all access attempts:

- Go to bucket â†’ Properties
- Server Access Logging
- Enable logging
- Target bucket: Create new logging bucket
- Prefix: `logs/`
- Save

All access attempts now logged to `logs/` folder!

### Encryption Cost

```
SSE-S3 (AES-256):
â””â”€ FREE (included)

SSE-KMS (Customer Master Key):
â”œâ”€ $1 per 10,000 requests
â”œâ”€ Better security
â””â”€ Good for sensitive data

Recommendation:
â”œâ”€ General data: SSE-S3 (free)
â”œâ”€ Sensitive data: SSE-KMS (small cost)
â””â”€ Regulated data: Compliance mode Object Lock
```

### Security Checklist

```
âœ… Default Encryption: Enabled
âœ… Bucket Policy: Deny unencrypted
âœ… Block Public Access: All ON
âœ… Versioning: Enabled
âœ… Access Logging: Enabled
âœ… MFA Delete: Enabled (critical buckets)
âœ… Object Lock: Enabled (compliance)

Result: Bank-level security! ğŸ”’
```

### Validation Checklist
- [ ] Default encryption enabled
- [ ] Bucket policy enforces encryption
- [ ] Block public access enabled
- [ ] Access logging configured
- [ ] Object Lock understanding
- [ ] Can verify encryption on objects

### Key Takeaway
âœ… Encryption = Free (SSE-S3)  
âœ… Bucket policies enforce security  
âœ… Block public access prevents accidents  
âœ… Logging enables audit trail  

---

### Lab 7: Static Website Hosting

**Objective:** Host a static website directly from S3.

**Why This Matters:**
- No web servers needed
- Cheap hosting ($0.023/GB)
- Good for blogs, portfolios, docs
- Scalable automatically

### Understanding S3 Website

```
Traditional Website:
You â†’ Web Server (EC2) â†’ Files
Cost: ~$10/month + maintenance

S3 Website:
You â†’ S3 Bucket â†’ Files
Cost: $0.023/GB/month
       + CloudFront for speed
```

### Step-By-Step Instructions

**Step 1: Create Website Files**

Create folder structure:
```
website/
â”œâ”€ index.html (home page)
â”œâ”€ error.html (error page)
â”œâ”€ style.css (styling)
â”œâ”€ script.js (javascript)
â””â”€ images/
   â””â”€ logo.png
```

Create index.html:
```html
<!DOCTYPE html>
<html>
<head>
    <title>My S3 Website</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <h1>Welcome to My S3 Website!</h1>
    <p>Hosted entirely on Amazon S3</p>
    <img src="images/logo.png" alt="Logo">
    <button onclick="clickMe()">Click Me!</button>
    <script src="script.js"></script>
</body>
</html>
```

Create style.css:
```css
body {
    font-family: Arial, sans-serif;
    max-width: 800px;
    margin: 50px auto;
    padding: 20px;
    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
}
h1 { color: white; }
p { color: #f0f0f0; }
button {
    padding: 10px 20px;
    font-size: 16px;
    cursor: pointer;
    background: white;
    border: none;
    border-radius: 5px;
}
```

Create script.js:
```javascript
function clickMe() {
    alert('Thanks for visiting my S3 website!');
}
```

Create error.html:
```html
<!DOCTYPE html>
<html>
<head>
    <title>Error 404</title>
</head>
<body>
    <h1>Page Not Found</h1>
    <p><a href="/">Go Home</a></p>
</body>
</html>
```

**Step 2: Upload Website Files**

```bash
# Upload all files
aws s3 sync website/ s3://my-first-s3-bucket-xxx/ \
  --acl public-read

# Verify upload
aws s3 ls s3://my-first-s3-bucket-xxx/ --recursive
```

**Step 3: Enable Static Website Hosting**

- Go to bucket â†’ Properties
- Static Website Hosting
- Enable hosting
- Index document: index.html
- Error document: error.html
- Save

Website endpoint appears:
```
http://my-first-s3-bucket-xxx.s3-website-us-east-1.amazonaws.com
```

**Step 4: Update Bucket Policy for Public Read**

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "PublicReadGetObject",
      "Effect": "Allow",
      "Principal": "*",
      "Action": "s3:GetObject",
      "Resource": "arn:aws:s3:::my-first-s3-bucket-xxx/*"
    }
  ]
}
```

Add to bucket policy:
- Go to bucket â†’ Permissions â†’ Bucket Policy
- Add above
- Save

**Step 5: Test Website**

- Visit: `http://my-first-s3-bucket-xxx.s3-website-us-east-1.amazonaws.com`
- See your website! ğŸŒ
- Click button, verify JavaScript works
- Try invalid URL, see error.html

**Step 6: Add Custom Domain (Optional)**

If you have a domain:
- Go to Route 53
- Create Alias record:
  - Name: yourdomain.com
  - Type: S3 Website Endpoint
  - Target: Your S3 website URL
  - Create

Now visit: yourdom ain.com (shows S3 website!)

**Step 7: Add CloudFront for Speed (Optional)**

```bash
# Create CloudFront distribution
# Origin: S3 website endpoint
# Caching: Default
# Create

# Get CloudFront URL:
# https://d123abc.cloudfront.net
```

Benefits:
- Files cached globally (faster)
- Reduces S3 costs
- Better performance worldwide

### Website Costs

```
S3 Hosting:
â”œâ”€ Storage: $0.023/GB/month
â”œâ”€ Requests: $0.0004 per 1,000 GETs
â””â”€ Monthly (100 GB): $2.30

With CloudFront:
â”œâ”€ S3: Same $2.30
â”œâ”€ CloudFront: $0.085/GB OUT
â””â”€ Total (1 GB/day): $5-10/month

Comparison:
EC2 Web Server: $10-20/month
S3 + CloudFront: $5-10/month
S3 only: $2-3/month

S3 = 50-80% cheaper! ğŸ’°
```

### Validation Checklist
- [ ] Website files created
- [ ] Files uploaded to S3
- [ ] Static website hosting enabled
- [ ] Bucket policy allows public read
- [ ] Can access website via endpoint URL
- [ ] JavaScript works
- [ ] Error page displays on 404
- [ ] CloudFront optional (understand benefits)

### Key Takeaway
âœ… S3 can host static websites  
âœ… Much cheaper than EC2  
âœ… Automatically scalable  
âœ… Add CloudFront for global speed  

---

### Lab 8: S3 Event Notifications & Lambda Integration

**Objective:** Trigger Lambda functions on S3 events.

**Why This Matters:**
- Automate tasks on file upload
- Process files automatically
- Serverless pipelines
- Real-time triggers

### Understanding S3 Events

```
Event Flow:

File Uploaded to S3
    â†“
S3 Event: ObjectCreated
    â†“
Lambda Function Triggered
    â†“
Function Processes File
    â†“
Results Stored
    â†“
Done! (No servers to manage)
```

### Step-By-Step Instructions

**Step 1: Create Lambda Function**

- Go to Lambda Console
- Click "Create Function"
- Name: `S3FileProcessor`
- Runtime: Python 3.11
- Execution role: Create new role with S3 access

**Step 2: Write Lambda Code**

Replace default code:
```python
import json
import boto3
import urllib.parse

s3 = boto3.client('s3')

def lambda_handler(event, context):
    """
    Process files uploaded to S3
    """
    
    # Get bucket and key from event
    bucket = event['Records'][0]['s3']['bucket']['name']
    key = urllib.parse.unquote_plus(event['Records'][0]['s3']['object']['key'])
    
    print(f"Processing {key} from {bucket}")
    
    try:
        # Get file from S3
        response = s3.get_object(Bucket=bucket, Key=key)
        content = response['Body'].read().decode('utf-8')
        
        # Process (example: convert to uppercase)
        processed = content.upper()
        
        # Save processed file
        output_key = f"processed/{key}"
        s3.put_object(
            Bucket=bucket,
            Key=output_key,
            Body=processed
        )
        
        return {
            'statusCode': 200,
            'body': json.dumps(f'Processed {key} -> {output_key}')
        }
    
    except Exception as e:
        print(f"Error: {e}")
        return {
            'statusCode': 500,
            'body': json.dumps(f'Error: {str(e)}')
        }
```

Click "Deploy"

**Step 3: Add S3 Permission to Lambda**

Lambda needs permission to access S3:
- Go to Lambda â†’ Configuration â†’ Permissions
- Execution role: Click role name
- Add inline policy:

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:GetObject",
        "s3:PutObject"
      ],
      "Resource": "arn:aws:s3:::my-first-s3-bucket-xxx/*"
    }
  ]
}
```

**Step 4: Create S3 Event Notification**

- Go to bucket â†’ Properties
- Event Notifications
- Click "Create Notification"
- Name: `FileUploadTrigger`
- Events: ObjectCreated (put, post, copy)
- Destination: Lambda Function
- Lambda Function: S3FileProcessor
- Create

**Step 5: Test the Integration**

```bash
# Upload a text file
echo "Hello World This Is A Test" > test.txt
aws s3 cp test.txt s3://my-first-s3-bucket-xxx/uploads/test.txt

# Lambda automatically triggered!
# Check for output file
aws s3 ls s3://my-first-s3-bucket-xxx/processed/

# Should see: processed/uploads/test.txt
aws s3 cp s3://my-first-s3-bucket-xxx/processed/uploads/test.txt - 
# Shows: HELLO WORLD THIS IS A TEST âœ…
```

**Step 6: Check Lambda Logs**

- Go to Lambda Console
- Click function
- Click "Monitor"
- View CloudWatch logs
- See function execution and output

### Real-World Use Cases

```
Use Case 1: Image Resize
â”œâ”€ User uploads image
â”œâ”€ Lambda triggered
â”œâ”€ Image resized
â”œâ”€ Resized uploaded to S3
â””â”€ Thumbnail created automatically

Use Case 2: Video Transcoding
â”œâ”€ User uploads video
â”œâ”€ Lambda triggers transcoding service
â”œâ”€ Multiple formats created
â”œâ”€ Stored back in S3
â””â”€ User downloads desired format

Use Case 3: Data Pipeline
â”œâ”€ CSV uploaded
â”œâ”€ Lambda validates format
â”œâ”€ Loads to DynamoDB
â”œâ”€ Sends confirmation email
â””â”€ All automated

Use Case 4: Malware Scanning
â”œâ”€ File uploaded
â”œâ”€ Lambda scans with ClamAV
â”œâ”€ If clean: Move to approved
â”œâ”€ If infected: Quarantine
â””â”€ Alert admin
```

### Cost of Event Notifications

```
S3 Events: FREE (included)
Lambda: 
â”œâ”€ First 1 million requests: FREE
â”œâ”€ After: $0.20 per million
â””â”€ Usually FREE tier covers you!

Typical monthly cost: $0 (within free tier)
```

### Validation Checklist
- [ ] Lambda function created
- [ ] Code written and deployed
- [ ] S3 permission added to Lambda
- [ ] Event notification configured
- [ ] File upload triggers Lambda
- [ ] Output file created in S3
- [ ] Can view Lambda logs

### Key Takeaway
âœ… S3 events trigger Lambda  
âœ… Serverless automation  
âœ… No servers to manage  
âœ… Usually free (within free tier)  

---

## POST-LAB 5-8 CLEANUP

**Keep resources (reuse in next labs):**

```bash
# No cleanup needed yet
# Versioning, encryption, website, Lambda all useful

# Cost so far:
# Storage: Minimal (< 1 GB = free tier)
# Events: FREE
# Lambda: FREE (within free tier)
# Total: $0 âœ…
```

---

## SECTION 3: PRODUCTION FEATURES (Labs 9-12)

### Lab 9: Replication (Cross-Region & Same-Region)

**Objective:** Automatically copy objects to another bucket.

**Why This Matters:**
- Disaster recovery
- Geographic redundancy
- Compliance requirements
- Data locality

### Understanding Replication

```
Replication Types:

1. Cross-Region Replication (CRR)
   â”œâ”€ Source: us-east-1
   â”œâ”€ Destination: eu-west-1
   â”œâ”€ Use: Disaster recovery, compliance
   â””â”€ Automatic replication

2. Same-Region Replication (SRR)
   â”œâ”€ Source: Bucket A (us-east-1)
   â”œâ”€ Destination: Bucket B (us-east-1)
   â”œâ”€ Use: Backup, analytics
   â””â”€ Automatic replication

Benefits:
âœ… Automatic copying
âœ… Low latency access
âœ… Compliance ready
âœ… Disaster recovery
```

### Step-By-Step Instructions

**Step 1: Create Destination Bucket**

- Go to S3 â†’ Create Bucket
- Name: `my-first-s3-bucket-backup-[RANDOM]`
- Region: Different region (e.g., eu-west-1)
- Create bucket
- Enable versioning on this bucket too!

**Step 2: Enable Versioning on Source**

Already enabled in Lab 1, verify:
- Source bucket â†’ Properties â†’ Versioning: Enabled âœ“

**Step 3: Create Replication Rule**

- Go to source bucket â†’ Management
- Replication
- Click "Create Replication Rule"
- Name: `BackupReplication`
- Priority: 1

**Step 4: Configure Source**

- Status: Enabled
- Source: This rule applies to:
  - All objects in this bucket
  - OR specific prefix: `replicable/`

**Step 5: Configure Destination**

- Destination bucket: Your backup bucket
- Destination region: Automatically shown (eu-west-1)
- Replication time control: Disabled (for now)

**Step 6: Set IAM Role**

- Click "Create new role"
- Role name: S3ReplicationRole
- This gives S3 permission to copy files

**Step 7: Enable Replication**

- Check "Accept the Amazon S3 Replication Time Control terms"
- Create Rule

**Step 8: Test Replication**

```bash
# Upload file to source bucket
echo "Test replication" > test-replica.txt
aws s3 cp test-replica.txt s3://my-first-s3-bucket-xxx/

# Wait 1-2 minutes for replication

# Check backup bucket
aws s3 ls s3://my-first-s3-bucket-backup-xxx/

# Should see test-replica.txt! âœ“
```

**Step 9: Verify Replication**

```bash
# Download from backup
aws s3 cp s3://my-first-s3-bucket-backup-xxx/test-replica.txt -

# Shows: Test replication âœ…

# Prove it's replicated, not just copied
# Delete from source
aws s3 rm s3://my-first-s3-bucket-xxx/test-replica.txt

# File still in backup!
aws s3 ls s3://my-first-s3-bucket-backup-xxx/
# Still shows test-replica.txt âœ“
```

### Replication with Prefix

```
Example: Only replicate specific folders

Rule 1: Replicate /backup/* to backup bucket
Rule 2: Replicate /archive/* to archive bucket

Only matching prefixes replicated! âœ…
```

### Replication Costs

```
Replication COST:
â”œâ”€ Replication requests: $0.02 per 1,000
â”œâ”€ Data transfer (inter-region): $0.02 per GB
â””â”€ Backup storage: $0.023 per GB/month

Example (100 GB/month to eu-west-1):
â”œâ”€ Storage: 100 Ã— $0.023 = $2.30
â”œâ”€ Transfer: 100 Ã— $0.02 = $2.00
â”œâ”€ Requests: $0.02
â””â”€ Total: $4.32/month

Worth it for disaster recovery! âœ…
```

### Validation Checklist
- [ ] Destination bucket created
- [ ] Both buckets have versioning
- [ ] Replication rule created
- [ ] Rule enabled
- [ ] File uploaded and replicated
- [ ] Can access file in backup bucket
- [ ] Deletion doesn't affect backup

### Key Takeaway
âœ… Replication = Automatic backup  
âœ… Cross-region = Disaster recovery  
âœ… Same-region = Backup + analytics  
âœ… Versioning required  

---

### Lab 10: CloudFront Integration

**Objective:** Use CloudFront to cache and accelerate S3 content.

**Why This Matters:**
- Speed up access globally
- Reduce S3 costs (fewer requests)
- Protection against traffic spikes
- Better user experience

### Understanding CloudFront

```
Without CloudFront:
User in Tokyo â†’ S3 in us-east-1
â””â”€ Long distance, slow, expensive

With CloudFront:
User in Tokyo â†’ CloudFront edge (Tokyo)
                â””â”€ Cached, fast! âœ…
           â†’ S3 (only first request)
                â””â”€ Slow but cached for future

Cost Benefit:
1,000 users requesting same file:
â”œâ”€ Without CF: 1,000 S3 requests Ã— $0.0004 = $0.40
â”œâ”€ With CF: 1 S3 request + 999 CF = $0.00024 + $0.01
â””â”€ Savings: 98%! ğŸ’°
```

### Step-By-Step Instructions

**Step 1: Create CloudFront Distribution**

- Go to CloudFront Console
- Click "Create Distribution"
- Choose "Web" (not RTMP)

**Step 2: Configure Origin**

- Origin Domain: my-first-s3-bucket-xxx.s3.amazonaws.com
- Protocol: HTTPS
- HTTP Port: 80
- HTTPS Port: 443
- Add Custom Header (optional):
  - Header Name: X-Custom-Header
  - Value: MyValue

**Step 3: Configure Caching**

- Cache Policy: Caching Optimized (default)
- TTL:
  - Default: 86400 (1 day)
  - Min: 1 (1 second)
  - Max: 31536000 (1 year)

**Step 4: Configure Behavior**

- Path Pattern: * (match all)
- Allowed HTTP Methods: GET, HEAD, OPTIONS
- Cache & Origin Request Policies: Default

**Step 5: Configure Settings**

- Price Class: Use all edge locations (or reduced for cost)
- Default Root Object: index.html (for websites)
- Compress Objects: Yes (faster delivery)
- Enable IPv6: Yes

**Step 6: Create Distribution**

- Click "Create Distribution"
- Status: Deploying (takes 5-10 minutes)
- Wait for "Enabled" status

**Step 7: Test CloudFront**

Once deployed:
```bash
# Get CloudFront domain
# https://d123abc.cloudfront.net

# First request (cache miss)
curl -w "\nTime: %{time_total}s\n" https://d123abc.cloudfront.net/index.html
# Shows time: ~500ms (slow, hitting origin)

# Second request (cache hit)
curl -w "\nTime: %{time_total}s\n" https://d123abc.cloudfront.net/index.html
# Shows time: ~50ms (fast, from edge!)

# Check headers
curl -I https://d123abc.cloudfront.net/index.html

# Shows:
# X-Cache: Hit from cloudfront (or Miss from cloudfront)
# X-Cache-Hits: Number of hits
```

**Step 8: Invalidate Cache (if needed)**

If you update a file and need immediate update:
```bash
# Invalidate specific path
aws cloudfront create-invalidation \
  --distribution-id D123ABC \
  --paths "/index.html"

# Invalidate all
aws cloudfront create-invalidation \
  --distribution-id D123ABC \
  --paths "/*"

# Invalidation takes ~5 minutes
```

### CloudFront Features

```
Security:
â”œâ”€ DDoS Protection (AWS Shield)
â”œâ”€ Web ACL (AWS WAF)
â””â”€ HTTPS everywhere

Performance:
â”œâ”€ 450+ edge locations globally
â”œâ”€ Automatic compression
â”œâ”€ Query string caching
â””â”€ HTTP/2 and HTTP/3 support

Cost Optimization:
â”œâ”€ Cache optimization
â”œâ”€ Request coalescing
â””â”€ Regional edge caches
```

### CloudFront Costs

```
CloudFront Data OUT:
â”œâ”€ First 10 TB: $0.085/GB
â”œâ”€ Next 40 TB: $0.080/GB
â”œâ”€ Next 100 TB: $0.060/GB
â””â”€ (decreases with volume)

Example (1 GB/day):
â”œâ”€ Direct S3: 1 GB Ã— 30 Ã— $0.09 = $2.70/month
â”œâ”€ With CloudFront: 1 GB Ã— 30 Ã— $0.085 = $2.55/month
â””â”€ Savings: $0.15/month (5%)

But: Massively faster! Usually worth it
```

### Validation Checklist
- [ ] CloudFront distribution created
- [ ] Distribution deployed (enabled)
- [ ] Can access via CloudFront domain
- [ ] First request slower (cache miss)
- [ ] Second request faster (cache hit)
- [ ] Can see X-Cache headers
- [ ] Understand invalidation

### Key Takeaway
âœ… CloudFront = Global caching  
âœ… Massively faster (10x+)  
âœ… Reduced S3 costs  
âœ… Protection against attacks  

---

### Lab 11: S3 Monitoring & Analytics

**Objective:** Monitor S3 usage and performance.

**Why This Matters:**
- Understand costs
- Detect unusual activity
- Optimize performance
- Capacity planning

### Understanding S3 Metrics

```
Metrics Available:

Size Metrics:
â”œâ”€ Total bucket size (GB)
â”œâ”€ Objects count
â””â”€ Size by storage class

Request Metrics:
â”œâ”€ GET requests
â”œâ”€ PUT requests
â”œâ”€ DELETE requests
â”œâ”€ Total requests
â””â”€ 4xx/5xx errors

Performance:
â”œâ”€ First byte latency
â”œâ”€ End-to-end latency
â””â”€ Request throughput
```

### Step-By-Step Instructions

**Step 1: Enable CloudWatch Metrics**

- Go to bucket â†’ Metrics
- Request Metrics â†’ Create Metric Configuration
- Name: `AllObjectsMetrics`
- Filter: (leave empty for all)
- Create Configuration
- Metrics now available in CloudWatch!

Takes 10-15 minutes to start showing data.

**Step 2: View CloudWatch Metrics**

- Go to CloudWatch â†’ Dashboards
- Create Dashboard: `S3Monitoring`
- Add Widget: Metric
  - Namespace: AWS/S3
  - Metric: NumberOfObjects
  - Statistic: Average
  - Period: 1 hour

Add more widgets:
- BucketSizeBytes
- AllRequests
- 4xxErrors
- 5xxErrors

**Step 3: Create Alarms**

Create alarm: High 4xx Errors
```bash
# 4xx errors might indicate misconfigured app
aws cloudwatch put-metric-alarm \
  --alarm-name S3High4xxErrors \
  --alarm-description "Alert if 4xx errors spike" \
  --metric-name 4xxErrors \
  --namespace AWS/S3 \
  --statistic Sum \
  --period 300 \
  --threshold 100 \
  --comparison-operator GreaterThanThreshold \
  --evaluation-periods 1 \
  --alarm-actions arn:aws:sns:us-east-1:123456789012:MyTopic
```

**Step 4: Enable Storage Class Analysis**

- Go to bucket â†’ Analytics
- Storage Class Analysis
- Add Analysis
- Name: `StorageAnalysis`
- Filter: (all objects)
- Create

Shows which objects would benefit from different storage class!

**Step 5: View S3 Inventory**

- Go to bucket â†’ Inventory
- Create Configuration
- Name: `BucketInventory`
- Destination Bucket: Same bucket (or different)
- Format: CSV
- Frequency: Daily
- Create

Daily CSV report showing all objects, sizes, dates!

**Step 6: Check Cost Allocation Tags**

- Go to bucket â†’ Tags
- Add cost allocation tags:
  - Key: `Environment`, Value: `Production`
  - Key: `Department`, Value: `Finance`

In AWS Billing, can now see costs by tag!

### Real-World Monitoring

```
Problem: S3 costs went up 50%

Investigation:
1. Check CloudWatch metrics
   â†’ See: Requests doubled
2. Check Storage Class Analysis
   â†’ See: Objects in STANDARD, could use GLACIER
3. Check Inventory
   â†’ See: Old logs not deleted
4. Check by Tags
   â†’ See: Finance department using most storage

Actions:
1. Investigate request spike (app bug?)
2. Move old logs to GLACIER (90% cost reduction)
3. Implement lifecycle policy
4. Work with Finance on data retention

Result: Back to normal costs! âœ…
```

### Validation Checklist
- [ ] CloudWatch metrics enabled
- [ ] Dashboard created with metrics
- [ ] Alarms configured
- [ ] Storage class analysis enabled
- [ ] Inventory configured
- [ ] Cost allocation tags added
- [ ] Can see metrics in CloudWatch

### Key Takeaway
âœ… CloudWatch shows usage  
âœ… Alarms alert on issues  
âœ… Storage analysis optimizes  
âœ… Tags track costs  

---

### Lab 12: S3 Cost Optimization

**Objective:** Reduce S3 costs through smart strategy.

**Why This Matters:**
- S3 can be expensive at scale
- Small optimizations = big savings
- Often can reduce 50%+ of costs

### Understanding Cost Drivers

```
S3 Cost Breakdown:

Storage (Biggest cost):
â”œâ”€ STANDARD: $0.023/GB/month
â”œâ”€ INTELLIGENT-TIERING: $0.0125/GB/month
â”œâ”€ GLACIER: $0.004/GB/month
â””â”€ DEEP_ARCHIVE: $0.00099/GB/month

Requests (Small cost):
â”œâ”€ PUT: $0.005 per 1,000
â”œâ”€ GET: $0.0004 per 1,000
â””â”€ LIST: $0.005 per 1,000

Data Transfer (Medium cost):
â”œâ”€ OUT to Internet: $0.09/GB
â”œâ”€ OUT to CloudFront: $0.085/GB (cheaper!)
â””â”€ Inter-region: $0.02/GB

Example Monthly Bill (100 GB):
â”œâ”€ STANDARD storage: 100 Ã— $0.023 = $2.30
â”œâ”€ Requests: ~$0.10
â”œâ”€ Data transfer OUT: $9.00
â””â”€ Total: $11.40
```

### Step-By-Step Instructions

**Step 1: Analyze Current Costs**

```bash
# Export to CSV
# AWS Billing Dashboard â†’ Create Report

# Shows:
# Date, Service, Amount
# 2024-01-15, Amazon S3, $50.00
# 2024-01-16, Amazon S3, $48.00
# Trending: ~$1,500/month
```

**Step 2: Implement Cost Optimization**

### Strategy 1: Lifecycle Policy (Already done in Lab 4)

```
Current: 100 GB STANDARD
Lifecycle:
â”œâ”€ 30 days old: STANDARD_IA (46% cost reduction)
â”œâ”€ 90 days old: GLACIER (83% cost reduction)
â””â”€ 1 year old: DEEP_ARCHIVE (96% cost reduction)

Savings: 60-80% typical
```

### Strategy 2: Use Intelligent-Tiering

```
Intelligent-Tiering:
â”œâ”€ Auto-moves between tiers based on access
â”œâ”€ No manual configuration
â”œâ”€ Tier Fee: $0.0025 per 1,000 objects
â”œâ”€ Good for unknown patterns

Cost:
100 GB with 10,000 objects:
â”œâ”€ Normal lifecycle: $2.30
â”œâ”€ Intelligent-tiering: $2.30 (storage) + $0.025 (tier fee) = $2.33
â””â”€
